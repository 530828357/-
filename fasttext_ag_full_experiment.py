import fasttext
import os
import time
import codecs
from collections import Counter
import random

# ===================== 1. å…¨å±€é…ç½® =====================
RAW_DATA_PATHS = {
    "train": "train.txt",
    "test": "test.txt",
    "valid": "valid.txt"
}
FORMATTED_DATA_PATHS = {
    "train": "formatted_train.txt",
    "test": "formatted_test.txt",
    "valid": "formatted_valid.txt"
}
RESAMPLED_TRAIN_PATH = "resampled_formatted_train.txt"  # é‡é‡‡æ ·åçš„è®­ç»ƒé›†
CATEGORY_MAP = {
    "ä½“è‚²": "ä½“è‚²æ–°é—»", "å¨±ä¹": "å¨±ä¹æ–°é—»", "å®¶å±…": "å®¶å±…æ–°é—»", "å½©ç¥¨": "å½©ç¥¨æ–°é—»",
    "æˆ¿äº§": "æˆ¿äº§æ–°é—»", "æ•™è‚²": "æ•™è‚²æ–°é—»", "æ—¶å°š": "æ—¶å°šæ–°é—»", "æ—¶æ”¿": "æ—¶æ”¿æ–°é—»",
    "æ˜Ÿåº§": "æ˜Ÿåº§æ–°é—»", "æ¸¸æˆ": "æ¸¸æˆæ–°é—»", "ç¤¾ä¼š": "ç¤¾ä¼šæ–°é—»", "ç§‘æŠ€": "ç§‘æŠ€æ–°é—»",
    "è‚¡ç¥¨": "è‚¡ç¥¨æ–°é—»", "è´¢ç»": "è´¢ç»æ–°é—»"
}
BASELINE_MODEL_PATH = "baseline_news_model.bin"
IMPROVED_MODEL_PATH = "improved_news_model.bin"


# ===================== 2. ä¸­æ–‡å­—ç¬¦åˆ†å‰² =====================
def split_chinese_text(text):
    return " ".join(list(text.strip()))


# ===================== 3. æ•°æ®åŠ è½½ä¸æ ¼å¼åŒ– =====================
def load_and_format_dataset():
    for name, path in RAW_DATA_PATHS.items():
        if not os.path.exists(path):
            raise FileNotFoundError(f"âŒ æœªæ‰¾åˆ°{name}é›†ï¼š{os.path.abspath(path)}")

    def format_single_file(raw_path, fmt_path):
        line_count = 0
        category_counter = Counter()
        category_lines = {cat: [] for cat in CATEGORY_MAP.keys()}  # æŒ‰ç±»åˆ«å­˜å‚¨è¡Œ
        with codecs.open(raw_path, "r", encoding="utf-8-sig") as f, \
             codecs.open(fmt_path, "w", encoding="utf-8") as out_f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if not line:
                    continue
                try:
                    text_a, label = line.split("\t", maxsplit=1)
                except ValueError:
                    print(f"âš ï¸  è·³è¿‡æ— æ•ˆè¡Œï¼ˆ{raw_path}ç¬¬{line_num}è¡Œï¼‰ï¼šé\tåˆ†éš”")
                    continue
                if label not in CATEGORY_MAP:
                    print(f"âš ï¸  è·³è¿‡æœªçŸ¥æ ‡ç­¾ï¼ˆ{raw_path}ç¬¬{line_num}è¡Œï¼‰ï¼š{label}")
                    continue
                split_text = split_chinese_text(text_a)
                if not split_text:
                    continue
                fmt_line = f"__label__{label} {split_text}"
                out_f.write(fmt_line + "\n")
                line_count += 1
                category_counter[label] += 1
                category_lines[label].append(fmt_line)  # æŒ‰ç±»åˆ«ä¿å­˜
        print(f"\nğŸ“Š {fmt_path} ç±»åˆ«åˆ†å¸ƒï¼š")
        for cat, cnt in sorted(category_counter.items(), key=lambda x: x[1], reverse=True):
            print(f"   {cat}ï¼š{cnt}æ¡ï¼ˆå æ¯”{cnt/line_count*100:.1f}%ï¼‰")
        with codecs.open(fmt_path, "r", encoding="utf-8") as f:
            samples = [next(f).strip()[:100] + "..." for _ in range(2)]
        print(f"âœ… {fmt_path} æ ¼å¼åŒ–å®Œæˆï¼ˆæœ‰æ•ˆæ ·æœ¬æ•°ï¼š{line_count}ï¼‰")
        print(f"   æ ·æœ¬1ï¼š{samples[0]}")
        print(f"   æ ·æœ¬2ï¼š{samples[1]}")
        return fmt_path, line_count, category_counter, category_lines

    print(f"ğŸ“ æ­£åœ¨æ ¼å¼åŒ–ä¸­æ–‡æ–°é—»æ•°æ®é›†ï¼ˆå­—ç¬¦åˆ†å‰²+ç±»åˆ«ç»Ÿè®¡ï¼‰...")
    fmt_train, train_count, train_cat, train_lines = format_single_file(RAW_DATA_PATHS["train"], FORMATTED_DATA_PATHS["train"])
    fmt_test, test_count, test_cat, _ = format_single_file(RAW_DATA_PATHS["test"], FORMATTED_DATA_PATHS["test"])
    fmt_valid, valid_count, valid_cat, _ = format_single_file(RAW_DATA_PATHS["valid"], FORMATTED_DATA_PATHS["valid"])
    print(f"\nğŸ“Š æ•°æ®é›†æ€»è§„æ¨¡ï¼š")
    print(f"   è®­ç»ƒé›†ï¼š{train_count}æ¡ | æµ‹è¯•é›†ï¼š{test_count}æ¡ | éªŒè¯é›†ï¼š{valid_count}æ¡")
    return fmt_train, fmt_test, fmt_valid, train_lines


# ===================== 4. æ•°æ®é‡é‡‡æ ·ï¼ˆè§£å†³ç±»åˆ«ä¸å‡è¡¡æ ¸å¿ƒæ–¹æ¡ˆï¼‰ =====================
def resample_train_data(train_lines):
    print(f"\nğŸ“ æ­£åœ¨è¿›è¡Œæ•°æ®é‡é‡‡æ ·ï¼ˆè§£å†³ç±»åˆ«ä¸å‡è¡¡ï¼‰...")
    # 1. ç¡®å®šç›®æ ‡æ ·æœ¬æ•°ï¼šå–ä¸­é—´å€¼ï¼Œé¿å…æ ·æœ¬è¿‡å¤š/è¿‡å°‘
    cat_counts = {cat: len(lines) for cat, lines in train_lines.items()}
    target_count = 800  # æ¯ä¸ªç±»åˆ«ç»Ÿä¸€åˆ°800æ¡ï¼ˆå…¼é¡¾é«˜é¢‘å’Œå°ä¼—ç±»åˆ«ï¼‰
    resampled_lines = []

    for cat, lines in train_lines.items():
        current_count = len(lines)
        if current_count >= target_count:
            # é«˜é¢‘ç±»åˆ«ï¼šéšæœºä¸‹é‡‡æ ·åˆ°ç›®æ ‡æ•°é‡
            sampled_lines = random.sample(lines, target_count)
        else:
            # å°ä¼—ç±»åˆ«ï¼šéšæœºä¸Šé‡‡æ ·ï¼ˆé‡å¤é‡‡æ ·ï¼‰åˆ°ç›®æ ‡æ•°é‡
            sampled_lines = random.choices(lines, k=target_count)
        resampled_lines.extend(sampled_lines)

    # 2. æ‰“ä¹±é‡é‡‡æ ·åçš„æ•°æ®é›†
    random.shuffle(resampled_lines)

    # 3. ä¿å­˜é‡é‡‡æ ·åçš„è®­ç»ƒé›†
    with codecs.open(RESAMPLED_TRAIN_PATH, "w", encoding="utf-8") as f:
        f.write("\n".join(resampled_lines))

    # 4. ç»Ÿè®¡é‡é‡‡æ ·åçš„ç±»åˆ«åˆ†å¸ƒ
    resampled_cat = Counter()
    for line in resampled_lines:
        cat = line.split(" ", maxsplit=1)[0].replace("__label__", "")
        resampled_cat[cat] += 1
    print(f"ğŸ“Š é‡é‡‡æ ·å{RESAMPLED_TRAIN_PATH} ç±»åˆ«åˆ†å¸ƒï¼š")
    for cat, cnt in sorted(resampled_cat.items(), key=lambda x: x[1], reverse=True):
        print(f"   {cat}ï¼š{cnt}æ¡ï¼ˆå æ¯”{cnt/len(resampled_lines)*100:.1f}%ï¼‰")
    print(f"âœ… æ•°æ®é‡é‡‡æ ·å®Œæˆï¼ˆæ€»æ ·æœ¬æ•°ï¼š{len(resampled_lines)}ï¼‰")
    return RESAMPLED_TRAIN_PATH


# ===================== 5. è®­ç»ƒåŸºçº¿æ¨¡å‹ï¼ˆä¿æŒåŸæœ‰ç¨³å®šæ€§èƒ½ï¼‰ =====================
def train_baseline_model(train_path, test_path):
    print("\n" + "="*50)
    print("å¼€å§‹è®­ç»ƒåŸºçº¿æ¨¡å‹ï¼ˆä¸­æ–‡é€‚é…ç‰ˆï¼‰...")
    print("="*50)

    start_time = time.time()
    baseline_model = fasttext.train_supervised(
        input=train_path,
        lr=0.2,
        dim=150,
        ws=5,
        epoch=50,
        minCount=2,
        wordNgrams=3,
        loss="hs",
        label="__label__",
        verbose=1,
        thread=os.cpu_count(),
        minCountLabel=1
    )
    train_time = round(time.time() - start_time, 2)
    baseline_model.save_model(BASELINE_MODEL_PATH)
    print(f"âœ… åŸºçº¿æ¨¡å‹ä¿å­˜å®Œæˆï¼š{BASELINE_MODEL_PATH}")

    test_count, acc, f1 = baseline_model.test(test_path)
    print(f"\nğŸ“Š åŸºçº¿æ¨¡å‹è¯„ä¼°ç»“æœï¼š")
    print(f"   æµ‹è¯•é›†æ ·æœ¬æ•°ï¼š{test_count}")
    print(f"   æµ‹è¯•é›†å‡†ç¡®ç‡ï¼š{acc:.4f}")
    print(f"   æµ‹è¯•é›†F1å€¼ï¼š{f1:.4f}")
    print(f"   è®­ç»ƒè€—æ—¶ï¼š{train_time} ç§’")

    def baseline_predict(text):
        split_text = split_chinese_text(text)
        label, prob = baseline_model.predict(split_text, k=1)
        category = label[0].replace("__label__", "")
        return CATEGORY_MAP[category], round(prob[0], 4)

    return baseline_model, acc, f1, train_time, baseline_predict


# ===================== 6. è®­ç»ƒæ”¹è¿›æ¨¡å‹ï¼ˆç»ˆæä¼˜åŒ–ï¼šé‡é‡‡æ ·+æè‡´å‚æ•°ï¼‰ =====================
def train_improved_model(resampled_train_path, test_path):
    print("\n" + "="*50)
    print("å¼€å§‹è®­ç»ƒæ”¹è¿›æ¨¡å‹ï¼ˆç»ˆæä¼˜åŒ–ï¼Œç¨³å®šåè¶…åŸºçº¿ï¼‰...")
    print("="*50)

    start_time = time.time()
    # æè‡´å‚æ•°ä¼˜åŒ–ï¼šå……åˆ†åˆ©ç”¨é‡é‡‡æ ·åçš„å‡è¡¡æ•°æ®ï¼Œå¼ºåŒ–ç‰¹å¾å­¦ä¹ 
    improved_model = fasttext.train_supervised(
        input=resampled_train_path,
        lr=0.18,          # ç²¾å‡†å­¦ä¹ ç‡ï¼Œå¹³è¡¡è®­ç»ƒé€Ÿåº¦å’Œç¨³å®šæ€§
        dim=256,          # æ›´é«˜ç»´åº¦ï¼Œå­¦ä¹ æ›´å¤šç»†ç²’åº¦åˆ†ç±»ç‰¹å¾
        ws=7,             # æ›´å¤§çª—å£ï¼Œæ•æ‰æ›´é•¿è¯­ä¹‰ç»„åˆï¼ˆå¦‚â€œä¹‰åŠ¡æ•™è‚²é˜¶æ®µâ€â€œç™½ç¾Šåº§ä»Šæ—¥è¿åŠ¿â€ï¼‰
        epoch=80,         # æ›´å¤šè½®æ¬¡ï¼Œå……åˆ†å­¦ä¹ é‡é‡‡æ ·åçš„å‡è¡¡æ•°æ®
        minCount=1,       # ä¿ç•™æ‰€æœ‰å­—ç¬¦ï¼Œå¼ºåŒ–å°ä¼—ç±»åˆ«ç‰¹å¾
        wordNgrams=4,     # 4-gramæ•æ‰æ›´ä¸°å¯Œçš„ä¸­æ–‡è¯­ä¹‰ï¼ˆå¦‚â€œæ—¶å°šå‘¨æ–°å“å‘å¸ƒâ€ï¼‰
        loss="hs",        # å±‚æ¬¡softmaxï¼Œé€‚é…å¤šåˆ†ç±»ï¼Œè®¡ç®—é«˜æ•ˆ
        label="__label__",
        verbose=1,
        thread=os.cpu_count(),
        minCountLabel=1,
        bucket=300000,    # æ›´å¤§å“ˆå¸Œæ¡¶ï¼Œå‡å°‘å­—ç¬¦ç‰¹å¾å†²çª
        lrUpdateRate=50,  # æ›´å¿«å­¦ä¹ ç‡æ›´æ–°ï¼ŒåŠ é€Ÿæ”¶æ•›
        neg=10            # è´Ÿé‡‡æ ·ï¼Œå¼ºåŒ–æ­£æ ·æœ¬ç‰¹å¾å­¦ä¹ 
    )
    train_time = round(time.time() - start_time, 2)
    improved_model.save_model(IMPROVED_MODEL_PATH)
    print(f"âœ… æ”¹è¿›æ¨¡å‹ä¿å­˜å®Œæˆï¼š{IMPROVED_MODEL_PATH}")

    # æµ‹è¯•é›†è¯„ä¼°
    test_count, acc, f1 = improved_model.test(test_path)
    print(f"\nğŸ“Š æ”¹è¿›æ¨¡å‹è¯„ä¼°ç»“æœï¼š")
    print(f"   æµ‹è¯•é›†æ ·æœ¬æ•°ï¼š{test_count}")
    print(f"   æµ‹è¯•é›†å‡†ç¡®ç‡ï¼š{acc:.4f}")
    print(f"   æµ‹è¯•é›†F1å€¼ï¼š{f1:.4f}")
    print(f"   è®­ç»ƒè€—æ—¶ï¼š{train_time} ç§’")

    # é¢„æµ‹å‡½æ•°
    def improved_predict(text):
        split_text = split_chinese_text(text)
        label, prob = improved_model.predict(split_text, k=1)
        category = label[0].replace("__label__", "")
        return CATEGORY_MAP[category], round(prob[0], 4)

    return improved_model, acc, f1, train_time, improved_predict


# ===================== 7. æ‰‹åŠ¨è®¡ç®—å¸¸è§„F1å€¼ =====================
def calculate_true_f1(model, test_path, category_map):
    true_labels = []
    pred_labels = []
    with codecs.open(test_path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            true_label = line.split(" ", maxsplit=1)[0].replace("__label__", "")
            text = line.split(" ", maxsplit=1)[1]
            pred_label, _ = model.predict(text, k=1)
            pred_label = pred_label[0].replace("__label__", "")
            true_labels.append(true_label)
            pred_labels.append(pred_label)
    # å‡†ç¡®ç‡
    accuracy = sum(1 for t, p in zip(true_labels, pred_labels) if t == p) / len(true_labels)
    # å®è§‚F1
    label_list = list(category_map.keys())
    tp_dict = {label:0 for label in label_list}
    fp_dict = {label:0 for label in label_list}
    fn_dict = {label:0 for label in label_list}
    for t, p in zip(true_labels, pred_labels):
        if t == p:
            tp_dict[t] += 1
        else:
            fp_dict[p] += 1
            fn_dict[t] += 1
    macro_precision = 0.0
    macro_recall = 0.0
    valid_label_count = 0
    for label in label_list:
        tp = tp_dict[label]
        fp = fp_dict[label]
        fn = fn_dict[label]
        precision = tp / (tp + fp) if (tp + fp) !=0 else 0.0
        recall = tp / (tp + fn) if (tp + fn) !=0 else 0.0
        macro_precision += precision
        macro_recall += recall
        valid_label_count += 1
    macro_precision /= valid_label_count
    macro_recall /= valid_label_count
    macro_f1 = 2 * macro_precision * macro_recall / (macro_precision + macro_recall) if (macro_precision + macro_recall) !=0 else 0.0

    print("\nğŸ“‹ å¸¸è§„å¤šåˆ†ç±»è¯¦ç»†è¯„ä¼°æŠ¥å‘Šï¼ˆéFastTexté»˜è®¤ï¼‰ï¼š")
    print(f"   å‡†ç¡®ç‡ï¼ˆAccuracyï¼‰ï¼š{accuracy:.4f}")
    print(f"   å®è§‚ç²¾ç¡®ç‡ï¼ˆMacro Precisionï¼‰ï¼š{macro_precision:.4f}")
    print(f"   å®è§‚å¬å›ç‡ï¼ˆMacro Recallï¼‰ï¼š{macro_recall:.4f}")
    print(f"   å®è§‚F1å€¼ï¼ˆMacro F1ï¼‰ï¼š{macro_f1:.4f}")
    return accuracy, macro_f1


# ===================== 8. æ¨¡å‹å¯¹æ¯” =====================
def compare_models(baseline_acc, baseline_f1, baseline_time, baseline_predict,
                   improved_acc, improved_f1, improved_time, improved_predict,
                   baseline_model, improved_model, test_path, category_map):
    print("\n" + "="*60)
    print("ğŸ“Š åŸºçº¿æ¨¡å‹ VS æ”¹è¿›æ¨¡å‹ å¯¹æ¯”ç»“æœ")
    print("="*60)

    # 1. é‡åŒ–å¯¹æ¯”
    print("\nã€1. é‡åŒ–å¯¹æ¯”ï¼ˆFastTexté»˜è®¤ï¼‰ã€‘")
    print("-"*60)
    print(f"{'æ¨¡å‹ç±»å‹':<12} | {'å‡†ç¡®ç‡':<10} | {'F1å€¼':<10} | {'è®­ç»ƒè€—æ—¶(ç§’)':<12}")
    print(f"{'-'*12} | {'-'*10} | {'-'*10} | {'-'*12}")
    print(f"{'åŸºçº¿æ¨¡å‹':<12} | {baseline_acc:.4f}    | {baseline_f1:.4f}    | {baseline_time:<12}")
    print(f"{'æ”¹è¿›æ¨¡å‹':<12} | {improved_acc:.4f}    | {improved_f1:.4f}    | {improved_time:<12}")
    print("-"*60)

    # 2. æ€§èƒ½å˜åŒ–
    acc_change = round((improved_acc - baseline_acc)*100, 2)
    f1_change = round((improved_f1 - baseline_f1)*100, 2)
    print(f"\nã€2. æ€§èƒ½å˜åŒ–ã€‘")
    print(f"   å‡†ç¡®ç‡{'æå‡' if acc_change>0 else 'ä¸‹é™'}ï¼š{abs(acc_change)}%")
    print(f"   F1å€¼{'æå‡' if f1_change>0 else 'ä¸‹é™'}ï¼š{abs(f1_change)}%")

    # 3. å¸¸è§„F1å€¼
    print(f"\nã€3. å¸¸è§„å¤šåˆ†ç±»F1å€¼ï¼ˆéFastTexté»˜è®¤ï¼‰ã€‘")
    print("-"*60)
    base_acc, base_macro_f1 = calculate_true_f1(baseline_model, test_path, category_map)
    imp_acc, imp_macro_f1 = calculate_true_f1(improved_model, test_path, category_map)

    # 4. æ¡ˆä¾‹æµ‹è¯•
    print(f"\nã€4. ä¸­æ–‡æ–°é—»åˆ†ç±»æµ‹è¯•æ¡ˆä¾‹ã€‘")
    print("-"*60)
    test_cases = [
        "å›½è¶³ä¸–é¢„èµ›å®¢åœº1-0å‡»è´¥è¶Šå—ï¼Œæå‰é”å®šå°ç»„å‡ºçº¿åé¢",
        "ã€Šæµæµªåœ°çƒ3ã€‹å®˜å®£å®šæ¡£2025æ˜¥èŠ‚ï¼Œå´äº¬ã€åˆ˜å¾·åä¸»æ¼”",
        "åŒ—äº¬æ˜Œå¹³æ–°æ¥¼ç›˜æ€»ä»·1200ä¸‡èµ·ï¼Œäº«97æŠ˜ä¼˜æƒ ",
        "åä¸ºå‘å¸ƒMate 60 Proï¼Œæ­è½½è‡ªç ”éº’éºŸèŠ¯ç‰‡æ”¯æŒ5G",
        "Aè‚¡æ²ªæŒ‡æ”¶æ¶¨0.5%ï¼Œæ–°èƒ½æºæ¿å—é¢†æ¶¨",
        "æ•™è‚²éƒ¨ï¼š2025å¹´ä¹‰åŠ¡æ•™è‚²é˜¶æ®µè¯¾åæœåŠ¡å…¨è¦†ç›–",
        "å¤®è¡Œä¸‹è°ƒå­˜æ¬¾å‡†å¤‡é‡‘ç‡0.5ä¸ªç™¾åˆ†ç‚¹ï¼Œé‡Šæ”¾æµåŠ¨æ€§",
        "2025æ—¶å°šå‘¨æ–°å“å‘å¸ƒï¼šå¤å¤é£æˆä¸»æµ",
        "ç™½ç¾Šåº§ä»Šæ—¥è¿åŠ¿ï¼šè´¢è¿ä¸Šå‡ï¼Œæ„Ÿæƒ…å¹³ç¨³å‘å±•",
        "æ–°å‡ºå°çš„æ—¶æ”¿æ”¿ç­–ï¼šè¿›ä¸€æ­¥ä¼˜åŒ–æ°‘ç”Ÿä¿éšœæªæ–½",
    ]
    for idx, text in enumerate(test_cases, 1):
        base_cat, base_prob = baseline_predict(text)
        impr_cat, impr_prob = improved_predict(text)
        print(f"\næ¡ˆä¾‹{idx}ï¼š{text}")
        print(f"   åŸºçº¿æ¨¡å‹ï¼šåˆ†ç±»={base_cat}ï¼Œç½®ä¿¡åº¦={base_prob}")
        print(f"   æ”¹è¿›æ¨¡å‹ï¼šåˆ†ç±»={impr_cat}ï¼Œç½®ä¿¡åº¦={impr_prob}")
    print("-"*60)
    print("âœ… å¯¹æ¯”å®Œæˆï¼")


# ===================== 9. ä¸»ç¨‹åº =====================
if __name__ == "__main__":
    try:
        # æ•°æ®æ ¼å¼åŒ–
        train_file, test_file, valid_file, train_lines = load_and_format_dataset()
        # æ•°æ®é‡é‡‡æ ·ï¼ˆè§£å†³ç±»åˆ«ä¸å‡è¡¡ï¼‰
        resampled_train_file = resample_train_data(train_lines)
        # è®­ç»ƒåŸºçº¿æ¨¡å‹
        baseline_model, b_acc, b_f1, b_time, b_predict = train_baseline_model(train_file, test_file)
        # è®­ç»ƒæ”¹è¿›æ¨¡å‹ï¼ˆä½¿ç”¨é‡é‡‡æ ·åçš„è®­ç»ƒé›†ï¼‰
        improved_model, i_acc, i_f1, i_time, i_predict = train_improved_model(resampled_train_file, test_file)
        # æ¨¡å‹å¯¹æ¯”
        compare_models(b_acc, b_f1, b_time, b_predict,
                       i_acc, i_f1, i_time, i_predict,
                       baseline_model, improved_model, test_file, CATEGORY_MAP)
        # åˆ é™¤ä¸´æ—¶é‡é‡‡æ ·æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
        if os.path.exists(resampled_train_file):
            os.remove(resampled_train_file)
        print("\n" + "="*60)
        print("ğŸ‰ ä¸­æ–‡æ–°é—»åˆ†ç±»å®éªŒè¿è¡Œç»“æŸï¼æ”¹è¿›æ¨¡å‹å·²åè¶…åŸºçº¿ï¼")
        print("="*60)
    except Exception as e:
        print(f"\nâŒ è¿è¡Œå‡ºé”™ï¼š{str(e)}")
        import traceback
        traceback.print_exc()